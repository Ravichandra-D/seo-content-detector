{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parsed HTML saved to: parsed_content.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def parse_html(html):\n",
    "    if not isinstance(html, str) or html.strip() == \"\":\n",
    "        return \"\", \"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag and title_tag.get_text(strip=True):\n",
    "        title = title_tag.get_text(strip=True)\n",
    "    else:\n",
    "        h1 = soup.find('h1')\n",
    "        title = h1.get_text(strip=True) if h1 else \"\"\n",
    "\n",
    "    \n",
    "    content = \"\"\n",
    "    for tag_name in ['article', 'main']:\n",
    "        tag = soup.find(tag_name)\n",
    "        if tag:\n",
    "            content = tag.get_text(\" \", strip=True)\n",
    "            break\n",
    "    if not content:\n",
    "        parts = [p.get_text(\" \", strip=True) for p in soup.find_all(['p','h2','h3']) if p.get_text(strip=True)]\n",
    "        content = \" \".join(parts)\n",
    "    \n",
    "    return title, clean_text(content)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "df['title'], df['body_text'] = zip(*df['html_content'].map(parse_html))\n",
    "df['word_count'] = df['body_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "parsed_path = \"parsed_content.csv\"\n",
    "df[['url','title','body_text','word_count']].to_csv(parsed_path, index=False)\n",
    "print(\"âœ… Parsed HTML saved to:\", parsed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db199da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"parsed_content.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ff2a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Cybersecurity is gaining more importance globa...</td>\n",
       "      <td>1743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Home Insights Blog Posts 11 Cyber Defense Tips...</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cybersecurity Best Practices CISA provides inf...</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           body_text  word_count  \n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         341  \n",
       "1  Cybersecurity is gaining more importance globa...        1743  \n",
       "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1091  \n",
       "3  Cybersecurity Best Practices CISA provides inf...         851  \n",
       "4                                                NaN           0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parsed_df = pd.read_csv(\"parsed_content.csv\")\n",
    "parsed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90bfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a333c4e5",
   "metadata": {},
   "source": [
    "# Text Preprocessing & Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b71fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features extracted and saved to: features.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lowercases, removes punctuation/numbers, and normalizes spaces.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Splits text into sentences using basic punctuation delimiters.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'[.!?;\\n]+', text)\n",
    "\n",
    "\n",
    "def flesch_reading_ease(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes a simple Flesch Reading Ease score.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    sentences = split_sentences(text)\n",
    "    words = re.findall(r\"\\w+\", text)\n",
    "    total_sentences = max(1, len(sentences))\n",
    "    total_words = max(1, len(words))\n",
    "    syllables = sum(len(re.findall(r'[aeiouy]+', w)) for w in words)\n",
    "    score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (syllables / total_words)\n",
    "    return round(score, 2)\n",
    "\n",
    "\n",
    "def extract_top_keywords(tfidf_matrix, feature_names, top_n=5):\n",
    "    \"\"\"\n",
    "    Extracts top N keywords per document from a TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    keywords_list = []\n",
    "    for row in tfidf_matrix:\n",
    "        coefs = row.toarray().ravel()\n",
    "        top_idx = np.argsort(coefs)[-top_n:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_idx if coefs[i] > 0]\n",
    "        keywords_list.append(\"|\".join(top_terms))\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "\n",
    "def build_features(input_csv_path: str, output_csv_path: str = \"features.csv\"):\n",
    "    \"\"\"\n",
    "    Builds NLP features from a parsed HTML dataset.\n",
    "    Saves results as a new CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    if \"body_text\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'body_text' column in input file.\")\n",
    "\n",
    "    df[\"clean_text\"] = df[\"body_text\"].apply(clean_text)\n",
    "\n",
    "    df[\"word_count\"] = df[\"clean_text\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "    df[\"sentence_count\"] = df[\"clean_text\"].apply(lambda x: len(split_sentences(x)))\n",
    "    df[\"flesch_reading_ease\"] = df[\"clean_text\"].apply(flesch_reading_ease)\n",
    "\n",
    "   \n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=3000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "    \n",
    "    if hasattr(vectorizer, \"get_feature_names_out\"):\n",
    "        feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    else:\n",
    "        feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "   \n",
    "    df[\"top_keywords\"] = extract_top_keywords(tfidf_matrix, feature_names)\n",
    "\n",
    "    \n",
    "    embeddings = tfidf_matrix.toarray()\n",
    "    df[\"embedding\"] = [emb[:20].round(6).tolist() for emb in embeddings]\n",
    "\n",
    "    \n",
    "    columns_to_save = [\n",
    "        \"url\",\n",
    "        \"word_count\",\n",
    "        \"sentence_count\",\n",
    "        \"flesch_reading_ease\",\n",
    "        \"top_keywords\",\n",
    "        \"embedding\"\n",
    "    ]\n",
    "    df[columns_to_save].to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\" Features extracted and saved to: {output_csv_path}\")\n",
    "    return df[columns_to_save]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_features = build_features(\"parsed_content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fd4aa",
   "metadata": {},
   "source": [
    "3. Duplicate Detection (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ab4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98de0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 81 embeddings, dim = 20\n",
      "\n",
      "ðŸ“Š SUMMARY REPORT\n",
      "Total pages analyzed : 81\n",
      "Duplicate pairs       : 74\n",
      "Thin content pages    : 34 (41.98%)\n",
      "âœ… Duplicate pairs saved â†’ duplicate_pairs.csv\n",
      "âœ… Thin content saved  â†’ thin_content.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def detect_duplicates(\n",
    "    input_csv_path: str,\n",
    "    sim_threshold: float = 0.80,\n",
    "    output_dupes_csv: str = \"duplicate_pairs.csv\",\n",
    "    output_thin_csv: str = \"thin_content.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects near-duplicate and thin content pages.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    if \"embedding\" not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain an 'embedding' column.\")\n",
    "\n",
    "   \n",
    "    def parse_embedding(x):\n",
    "        try:\n",
    "            \n",
    "            return np.array(eval(x), dtype=float)\n",
    "        except Exception:\n",
    "            return np.array([])\n",
    "\n",
    "    df[\"embedding_array\"] = df[\"embedding\"].apply(parse_embedding)\n",
    "\n",
    "    \n",
    "    embeddings = np.vstack(df[\"embedding_array\"].values)\n",
    "    print(f\" Loaded {len(df)} embeddings, dim = {embeddings.shape[1]}\")\n",
    "\n",
    "  \n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    \n",
    "    dup_pairs = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim_score = sim_matrix[i, j]\n",
    "            if sim_score >= sim_threshold:\n",
    "                dup_pairs.append({\n",
    "                    \"url1\": df.loc[i, \"url\"],\n",
    "                    \"url2\": df.loc[j, \"url\"],\n",
    "                    \"similarity\": round(float(sim_score), 3)\n",
    "                })\n",
    "\n",
    "    df_dupes = pd.DataFrame(dup_pairs)\n",
    "    df_dupes.to_csv(output_dupes_csv, index=False)\n",
    "\n",
    "   \n",
    "    df[\"is_thin\"] = df[\"word_count\"].apply(lambda x: x < 500)\n",
    "    df_thin = df[[\"url\", \"word_count\", \"is_thin\"]]\n",
    "    df_thin.to_csv(output_thin_csv, index=False)\n",
    "\n",
    "    \n",
    "    total_pages = len(df)\n",
    "    duplicate_pairs = len(df_dupes)\n",
    "    thin_pages = df[\"is_thin\"].sum()\n",
    "    thin_percent = round((thin_pages / total_pages) * 100, 2)\n",
    "\n",
    "    print(\"\\nðŸ“Š SUMMARY REPORT\")\n",
    "    print(f\"Total pages analyzed : {total_pages}\")\n",
    "    print(f\"Duplicate pairs       : {duplicate_pairs}\")\n",
    "    print(f\"Thin content pages    : {thin_pages} ({thin_percent}%)\")\n",
    "    print(f\"âœ… Duplicate pairs saved â†’ {output_dupes_csv}\")\n",
    "    print(f\"âœ… Thin content saved  â†’ {output_thin_csv}\")\n",
    "\n",
    "    return df_dupes, df_thin\n",
    "\n",
    "\n",
    "df_dupes, df_thin = detect_duplicates(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a1bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d9495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Baseline Accuracy (word_count only): 0.56\n",
      "\n",
      "ðŸ§¾ Model Performance:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Overall Accuracy: 1.00\n",
      "Weighted F1-score: 1.00\n",
      "Baseline Accuracy: 0.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top Features:\n",
      "               Feature  Importance\n",
      "0           word_count         0.0\n",
      "1       sentence_count         0.0\n",
      "2  flesch_reading_ease         0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STEP 4: CONTENT QUALITY SCORING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_quality_classifier(input_csv_path: str):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model to classify content quality (Low / Medium / High)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    required_cols = [\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    \n",
    "    def label_quality(row):\n",
    "        if row[\"word_count\"] > 1500 and 50 <= row[\"flesch_reading_ease\"] <= 70:\n",
    "            return \"High\"\n",
    "        elif row[\"word_count\"] < 500 or row[\"flesch_reading_ease\"] < 30:\n",
    "            return \"Low\"\n",
    "        else:\n",
    "            return \"Medium\"\n",
    "\n",
    "    df[\"quality_label\"] = df.apply(label_quality, axis=1)\n",
    "\n",
    "    \n",
    "    X = df[[\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]]\n",
    "    y = df[\"quality_label\"]\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    \n",
    "    def baseline_predict(row):\n",
    "        if row[\"word_count\"] > 1500:\n",
    "            return \"High\"\n",
    "        elif row[\"word_count\"] < 500:\n",
    "            return \"Low\"\n",
    "        else:\n",
    "            return \"Medium\"\n",
    "\n",
    "    y_pred_baseline = X_test.apply(baseline_predict, axis=1)\n",
    "    baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "    print(f\" Baseline Accuracy (word_count only): {baseline_acc:.2f}\")\n",
    "\n",
    "   \n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=150)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "   \n",
    "    print(\"\\n Model Performance:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    print(f\"Overall Accuracy: {acc:.2f}\")\n",
    "    print(f\"Weighted F1-score: {f1:.2f}\")\n",
    "    print(f\"Baseline Accuracy: {baseline_acc:.2f}\")\n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Low\", \"Medium\", \"High\"],\n",
    "                yticklabels=[\"Low\", \"Medium\", \"High\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"Importance\": model.feature_importances_\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "        print(\"\\nðŸ” Top Features:\")\n",
    "        print(feature_importance.head(3))\n",
    "    else:\n",
    "        print(\"âš ï¸ Model does not support feature importance.\")\n",
    "\n",
    "    return model, df\n",
    "#Example\n",
    "model, df_qualified = train_quality_classifier(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d36cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6f2865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from textstat) (52.0.0.post20210125)\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: nltk in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from textstat) (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from nltk->textstat) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from nltk->textstat) (8.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from nltk->textstat) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from nltk->textstat) (2021.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravichandra d\\anaconda3\\lib\\site-packages (from click->nltk->textstat) (0.4.4)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.16.0 textstat-0.7.10\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc081ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"404 Client Error: OK for url: https://edition.cnn.com/2024/06/03/politics/us-mexico-border-immigration-executive-actions/index.html\",\n",
      "  \"url\": \"https://www.cnn.com/2024/06/03/politics/us-mexico-border-immigration-executive-actions/index.html\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textstat\n",
    "import json\n",
    "\n",
    "def analyze_url(url, reference_csv=None, vectorizer=None, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Analyze a URL: scrape page, extract features, compute quality, detect duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): target URL\n",
    "    - reference_csv (str): optional CSV with previous texts\n",
    "    - vectorizer: optional pre-fitted TF-IDF vectorizer\n",
    "    - threshold (float): similarity threshold for duplicates\n",
    "    \n",
    "    Returns:\n",
    "    - dict: features, quality, similar URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "       \n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/118.0.5993.90 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        text = \" \".join(soup.get_text(separator=\" \").split())\n",
    "\n",
    "        \n",
    "        word_count = len(text.split())\n",
    "        readability = round(textstat.flesch_reading_ease(text), 2)\n",
    "        is_thin = word_count < 500\n",
    "\n",
    "        \n",
    "        if word_count > 1500 and 50 <= readability <= 70:\n",
    "            quality_label = \"High\"\n",
    "        elif word_count < 500 or readability < 30:\n",
    "            quality_label = \"Low\"\n",
    "        else:\n",
    "            quality_label = \"Medium\"\n",
    "\n",
    "       \n",
    "        similar_to = []\n",
    "        if reference_csv:\n",
    "            df_ref = pd.read_csv(reference_csv)\n",
    "            if \"clean_text\" not in df_ref.columns:\n",
    "                raise ValueError(\"Reference CSV must have 'clean_text' column.\")\n",
    "            if vectorizer is None:\n",
    "                vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "            tfidf_ref = vectorizer.fit_transform(df_ref[\"clean_text\"])\n",
    "            tfidf_new = vectorizer.transform([text])\n",
    "            sim_matrix = cosine_similarity(tfidf_new, tfidf_ref).flatten()\n",
    "            df_ref[\"similarity\"] = sim_matrix\n",
    "            similar_to = df_ref[df_ref[\"similarity\"] > threshold][[\"url\", \"similarity\"]].to_dict(orient=\"records\")\n",
    "\n",
    "       \n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"word_count\": word_count,\n",
    "            \"readability\": readability,\n",
    "            \"quality_label\": quality_label,\n",
    "            \"is_thin\": is_thin,\n",
    "            \"similar_to\": similar_to\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"url\": url}\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "result = analyze_url(\n",
    "    \"https://www.cnn.com/2024/06/03/politics/us-mexico-border-immigration-executive-actions/index.html\",\n",
    "    reference_csv=\"parsed_content.csv\"  \n",
    ")\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722eaaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
